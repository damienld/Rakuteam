# -*- coding: utf-8 -*-
"""4_Model_Reseaux_Text.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TWC2hdXarDgK2d8rVHIh5eIrnf9FJHfW
"""

#from keras.saving.saved_model.load import load
import pandas as pd
import constants
import matplotlib.pyplot as plt
plt.style.use('ggplot')


#PARAMETERS before launching
#define if we should keep the part "X_test_challenge" from the loaded file or ignore it
Ignore_X_test_challenge = True

#CHARGEMENT des fichiers
#dataset_cleaned.csv dot avoir été généré depuis le notebook "cleaning"
import re  
import requests
import io
import pickle
"""
url = "https://raw.githubusercontent.com/JulienJ-44/rakuteam/main/Datasets/dataset_challenge_cleaned.csv"
download = requests.get(url).content
df=pd.read_csv(io.StringIO(download.decode('utf-8')), index_col=0)
df=df.drop("Unnamed: 0.1", axis=1)
df.head()

if (Ignore_X_test_challenge):
    df =df.dropna(subset=['prdtypecode'])
print("Taille du dataframe:",len(df))
#y = pd.read_csv(\"/Drive/My Drive/Colab/Y_train_CVw08PX.csv\", index_col=0) \r\n",
#path = '/Drive/My Drive/Projet Rakuten'
#df = pd.read_csv(f'{path}/dataset_cleaned.csv', index_col=0) 
df = df.replace({'prdtypecode': {10: 1, 2280:2,   50:3, 1280:4, 2705:5, 2522:6, 2582:7, 1560:8, 1281:9, 1920:10, 2403:11,
       1140:12, 2583:13, 1180:14, 1300:15, 2462:16, 1160:17, 2060:18,   40:19,   60:20, 1320:21, 1302:22,
       2220:23, 2905:24, 2585:25, 1940:26, 1301:0}})
#valeurs MANQUANTES
df['description']=df['description'].fillna("")
df['designation']=df['designation'].fillna("")
#df['description']=df['description'].replace({'n°': 'numéro '}, regex=True)
#df['designation']=df['designation'].replace({'n°': 'numéro '}, regex=True)
df['description']=df['description'].replace({"'": ' '}, regex=True)
df['designation']=df['designation'].replace({"'": ' '}, regex=True)
#classes_codes = (y['prdtypecode'].value_counts().index.tolist())
df.head()

#on concatène les 2 champs texte
df['sentences'] = df['designation'] + " " + df['description']

from sklearn.model_selection import train_test_split
#création dataset train / test 


#classes_codes = (y['prdtypecode'].value_counts().index.tolist())

import numpy as np
desi="jeu chaise longue pcs textilène noir noir"
desc="cet ensemble deux chaises longues haute qualité petite table assortie idéal passer après-midi détente jardin camping chaises longues durables faciles nettoyer revêtues textilène doux confortable construites cadre acier robuste deux chaises longues d'extérieur durables résistants intempéries l'ensemble complété table assortie élégant dessus table verre lequel pouvez mettre boissons garder livre téléphone portée main cet ensemble excellent ajout espace vie extérieur couleur noir matériau chaise longue structure acier 43 siège dossier textilène dimensions chaise longue 200 58 32 cm dimensions table 30 30 295 cm hauteur dossier réglable 62/72/80/89/95 cm comprend table dessus table verre mm d'épaisseur résistance intempéries matériel polyester 30 pvc 70"

if (Ignore_X_test_challenge):
  y = df['prdtypecode'].values
  sentences_train, sentences_test, y_train, y_test = train_test_split(
        df['sentences'], y, test_size=0.2, random_state=123)
else:
  df_train = df.dropna(subset=["prdtypecode"], axis=0)
  df_test = df[np.isnan(df["prdtypecode"])]
  y_train = df_train['prdtypecode'].values
  y_test = None
  sentences_train = df_train['sentences']
  sentences_test = df_test['sentences']
"""

from cleaning import clean_manualdata
desi="jeu chaise longue pcs textilène noir noir"
desc="cet ensemble deux chaises longues haute qualité petite table assortie idéal passer après-midi détente jardin camping chaises longues durables faciles nettoyer revêtues textilène doux confortable construites cadre acier robuste deux chaises longues d'extérieur durables résistants intempéries l'ensemble complété table assortie élégant dessus table verre lequel pouvez mettre boissons garder livre téléphone portée main cet ensemble excellent ajout espace vie extérieur couleur noir matériau chaise longue structure acier 43 siège dossier textilène dimensions chaise longue 200 58 32 cm dimensions table 30 30 295 cm hauteur dossier réglable 62/72/80/89/95 cm comprend table dessus table verre mm d'épaisseur résistance intempéries matériel polyester 30 pvc 70"
dfmanual=clean_manualdata(desi,desc)
dfmanual['sentences'] = dfmanual['designation'] + " " + dfmanual['description']
sentences_test = dfmanual['sentences']


#print("sentences train:", len(sentences_train))
print("sentences test:", sentences_test)
"""
from keras.utils import np_utils
y_train = np_utils.to_categorical(y_train, dtype = 'int') # Veiller à n'exécuter cette instruction qu'une seule fois
if (not (y_test is None)):
  y_test = np_utils.to_categorical(y_test, dtype = 'int')   # Veiller à n'exécuter cette instruction qu'une seule fois
"""
#Création du vocabulaire
#Tokenization de X_train / X_test et transformation en séquences de mots du vocabulaire
#   Vocabulary(Keys only) dans tokenizer : ["'", "'cm'", "'couleur'", "'taille'", "'piscine'", "'plus'", "'peut'", "'haute'", "'qualité'", "'être'", "'1", "'dimensions'", ...
#   Article(texte + description) dans sentences_test/sentences_train: ['jeu', 'chaise', 'longue', 'pcs', 'textilène', 'noir', 'noir']
#   Représentation de l'article dans X_test/X_train: [21, 288, 435, 494, 5449, 96, 96]

#Note: Pay close attention to the difference between this technique and the X_train that was produced by scikit-learn’s CountVectorizer.
#With CountVectorizer, we had stacked vectors of word counts, and each vector was the same length (the size of the total corpus vocabulary). 
#With Tokenizer, the resulting vectors equal the length of each text, and the numbers don’t denote counts
#   , but rather correspond to the word values from the dictionary tokenizer.word_index.
from keras.preprocessing.text import Tokenizer
"""tokenizer = Tokenizer(num_words=20000)#default was 10000/best 20000
tokenizer.fit_on_texts(sentences_train)
# saving
with open(constants.path+'tokenizer_dnn.pickle', 'wb') as handle:
    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"""

# loading
with open(constants.path+'tokenizer_dnn.pickle', 'rb') as handle:
    tokenizer = pickle.load(handle)
#X_train = tokenizer.texts_to_sequences(sentences_train)
X_test = tokenizer.texts_to_sequences(sentences_test)
vocab_size = len(tokenizer.word_index) + 1  # Adding 1 because of reserved 0 index
#print(sentences_train.shape)
#print(len(X_train))

#On complète chaque représentation d'article sous la forme [21, 288, 435, 494, 5449, 96, 96] en [21, 288, 435, 494, 5449, 96, 96, 0, 0, 0, ...]
#pour conserver des tailles de phrases similaires
from keras.preprocessing.sequence import pad_sequences
maxlen = 400#defautl was 250, best 400
#X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)
X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)
#print("X_train", len(X_train))
print("X_test", len(X_test))

"""
t0 = time()
embedding_dim = 100

model = Sequential()
model.add(layers.Embedding(input_dim=vocab_size, 
                           output_dim=embedding_dim, 
                           input_length=maxlen))
#model.add(layers.GlobalMaxPooling1D())
#model.add(Dropout(0.25))
model.add(layers.Flatten())
#model.add(layers.Dropout(0.25))
#model.add(layers.Dense(300, activation='relu'))
model.add(layers.Dense(100, activation='relu'))
model.add(layers.Dropout(0.5)) #78.5 sans Dropout; 78.8 avec 0.25; 79.0 avec 0.5;
model.add(layers.Dense(27, activation='softmax'))
# last_layer = Dense(units = 27,
#                      kernel_initializer ='normal',
#                      activation ='softmax')
model.compile(optimizer=keras.optimizers.Adam(lr=0.001), #0.001
              loss='categorical_crossentropy',
              metrics=['accuracy'])
model.summary()

print('Time for DNN1: {} mins'.format(round((time() - t0) / 60, 2)))

if (Ignore_X_test_challenge):
  history = model.fit(x=X_train, y=y_train,
                    epochs=2,#5
                    #callbacks=[best_model],#reduce_lr, 
                    batch_size=200,
                    validation_data=(X_test, y_test))
else:
  history = model.fit(X_train, y_train,
                    epochs=5,#5
                    batch_size=200)
"""
from keras.models import load_model
#from keras.utils.data_utils import get_file

#url = "https://drive.google.com/u/1/open?id=1es-p47fLuDD-BLJQqdOYJgLbkF6xoubp" 
#download = requests.get(url).content    
#print("downloaded hdf5")
#model = load_model(download)
model = load_model("text-dnn.hdf5") 
print("model loaded")   
ypred_proba=model.predict(X_test)
"""score = model.evaluate(X_test, y_test, verbose = 0) 
print('Test loss:', score[0]) 
print('Test accuracy:', score[1])"""
print(ypred_proba[0])
